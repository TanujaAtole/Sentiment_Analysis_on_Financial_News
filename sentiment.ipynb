{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0                                                  1\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "Index([0, 1], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data without assuming the first row is column headers\n",
    "df = pd.read_csv('all-data.csv', header=None, encoding='latin1')\n",
    "\n",
    "# Print the first few rows and the columns to inspect the data\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                            article\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "# Rename the unnamed column to 'article' for clarity\n",
    "df.columns = ['label', 'article']\n",
    "\n",
    "\n",
    "\n",
    "# Check the output\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'article'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                            article\n",
      "0   neutral  Technopolis plans to develop in stages an area...\n",
      "1  negative  The international electronic industry company ...\n",
      "2  positive  With the new production plant the company woul...\n",
      "3  positive  According to the company 's updated strategy f...\n",
      "4  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                            article  \\\n",
      "0   neutral  Technopolis plans to develop in stages an area...   \n",
      "1  negative  The international electronic industry company ...   \n",
      "2  positive  With the new production plant the company woul...   \n",
      "3  positive  According to the company 's updated strategy f...   \n",
      "4  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  technopolis plan develop stage area less 0,000...  \n",
      "1  international electronic industry company elco...  \n",
      "2  new production plant company would increase ca...  \n",
      "3  according company updated strategy year 009-20...  \n",
      "4  financing aspocomp growth aspocomp aggressivel...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('all-data.csv', encoding='latin1')\n",
    "\n",
    "# Rename the unnamed column to 'article' for clarity\n",
    "df.columns = ['label', 'article']  # 'label' for the first column, 'article' for the second column containing text\n",
    "\n",
    "# Show the first few rows to verify the column names\n",
    "print(df.head())\n",
    "\n",
    "# Initialize NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and convert to lower case\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization and stopword removal\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'article' column\n",
    "df['processed_text'] = df['article'].apply(preprocess_text)\n",
    "\n",
    "# Verify that preprocessing worked\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "neutral     2878\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check how the labels are mapped\n",
    "print(df['label'].value_counts())  # Check the distribution of sentiment labels\n",
    "\n",
    "# Double-check the mapping if you haven't already\n",
    "y = df['label'].map({'positive': 1, 'negative': 0, 'neutral': 2})\n",
    "\n",
    "# Split dataset into features and labels\n",
    "X = df['processed_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into features and labels\n",
    "X = df['processed_text']\n",
    "y = df['label']  # Assuming 'sentiment' contains the labels (positive, negative, neutral)\n",
    "\n",
    "# Convert the labels into numeric values (if not already done)\n",
    "y = y.map({'positive': 1, 'negative': 0, 'neutral': 2})\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences to ensure they are of the same length\n",
    "max_sequence_length = 100  # You can adjust this based on your dataset\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "61/61 [==============================] - 30s 425ms/step - loss: 0.8898 - accuracy: 0.6164 - val_loss: 0.8202 - val_accuracy: 0.6285\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 26s 424ms/step - loss: 0.6149 - accuracy: 0.7314 - val_loss: 0.8594 - val_accuracy: 0.6966\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 26s 433ms/step - loss: 0.3303 - accuracy: 0.8725 - val_loss: 0.9310 - val_accuracy: 0.6760\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 26s 418ms/step - loss: 0.1758 - accuracy: 0.9422 - val_loss: 1.1571 - val_accuracy: 0.7131\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 29s 469ms/step - loss: 0.0903 - accuracy: 0.9727 - val_loss: 1.3490 - val_accuracy: 0.7141\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: positive, negative, neutral\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 29ms/step - loss: 1.3490 - accuracy: 0.7141\n",
      "Test Accuracy: 0.7141\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 545ms/step\n",
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(news_text):\n",
    "    # Preprocess the input text\n",
    "    processed_text = preprocess_text(news_text)\n",
    "    \n",
    "    # Convert text to sequence and pad\n",
    "    seq = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Predict sentiment\n",
    "    pred = model.predict(padded_seq)\n",
    "    \n",
    "    # Get the label with the highest probability\n",
    "    sentiment_labels = ['negative', 'positive', 'neutral']\n",
    "    sentiment = sentiment_labels[pred.argmax()]\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "new_article = 'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .'\n",
    "predicted_sentiment = predict_sentiment(new_article)\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data\\\\tokenizers\\\\punkt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.find('tokenizers/punkt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
